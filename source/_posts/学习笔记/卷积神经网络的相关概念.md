---
title: 卷积神经网络(CNN)的相关概念
categories:
  - 学习笔记
tags: 
  - 卷积神经网络
abbrlink: 6ff8
---

![](https://ws2.sinaimg.cn/large/006tNc79ly1fzpytupuhoj30xa0d04dz.jpg)

## 传统神经网络存在的问题

![传统神经网络](https://ws4.sinaimg.cn/large/006tNbRwgy1fycb7xeo3hj30lz0asn4a.jpg)

说卷积神经网络前，我们要先说一下传统神经网络存在的一些问题，上图是一个典型的传统神经网络的示例图。设想一个场景，假设我们要训练的的样本图片是100x100（像素）的，那么整张图片总共就是有10000个像素，那么在定义一个 传统神经网络的时候，输入层(input layer)就需要有1w个神经元，那么如果我们的中间的隐藏层(hidden layer)也需要有1w个神经元，那么总共需要的参数（权值）就高达1亿个（1w*1w），试想一下，这还只是一张100x100的图片就需要这么多的参数，如果图片更大之后呢，可想而知整个神经网络的计算量有多恐怖。当然，一旦权重多了之后，则必须要有足够量的样本进行训练，否则就会出现[过拟合](https://yueyong.info/p/f2b.html)的现象。因此我们可以知道，传统神经网络有以下两个问题：

* 权值太多，计算量太大
* 权值太多，如果没有大量样本支撑则会出现过拟合现象

<!--more-->

## 卷积神经网络

### 卷积

#### 什么是卷积？

在了解卷积神经网络之前我们需要知道什么是卷积。对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器(filter)，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。

![滤波器](https://ws2.sinaimg.cn/large/006tNbRwgy1fycciisv4oj30co06xt9q.jpg)

举个具体的例子。比如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据。

![](https://ws2.sinaimg.cn/large/006tNbRwgy1fyccjacwonj30g70csafb.jpg)

 分解下上图

>  ![](https://ws2.sinaimg.cn/large/006tNbRwgy1fycckt9eo4j302602ut8u.jpg)对应位置上是数字先相乘后相加![](https://ws3.sinaimg.cn/large/006tNbRwgy1fyccld8qxbj302a02e0st.jpg) = ![](https://ws4.sinaimg.cn/large/006tNbRwgy1fycclulpizj301f017744.jpg)

中间滤波器filter与数据窗口做内积，其具体计算过程则是：4x0 + 0x0 + 0x0 + 0x0 + 0x1 + 0x1 + 0x0 + 0x1 + -4x2 = -8

#### 图像上的卷积

在下图对应的计算过程中，输入是一定区域大小(width*height)的数据，和滤波器filter（带着一组固定权重的神经元）做内积后等到新的二维数据。

如下图所示:

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fyccslh0vpj30hv0b1wq3.jpg)

具体来说，左边是图像输入，中间部分就是滤波器filter（带着一组固定权重的神经元），不同的滤波器filter会得到不同的输出数据，比如颜色深浅、轮廓。相当于如果想提取图像的不同特征，则用不同的滤波器filter，提取想要的关于图像的特定信息：颜色深浅或轮廓。用一句话解释不同滤波器之间的差异就是：**一千个读者就有一千个哈姆雷特**

### 什么是卷积神经网络？

**卷积神经网络（Convolutional Neural Network, CNN）**是一种[前馈神经网络](https://baike.baidu.com/item/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。卷积神经网络与普通神经网络非常相似，它们都由具有可学习的权重和偏置常量(biases)的神经元组成。每个神经元都接收一些输入，并做一些点积计算，输出是每个分类的分数，普通神经网络里的一些计算技巧到这里依旧适用。但是卷积神经网络默认输入是图像，可以让我们把特定的性质编码入网络结构，使是我们的前馈函数更加有效率，并减少了大量参数。

**具有三维体积的神经元(3D volumes of neurons) **

卷积神经网络利用输入是图片的特点，把神经元设计成三个维度 ： width, height, depth(注意这个depth不是神经网络的深度，而是用来描述神经元的) 。比如输入的图片大小是 32 × 32 × 3 (rgb)，那么输入神经元就也具有 32×32×3 的维度。下面是图解：

![卷积神经网络](https://ws1.sinaimg.cn/large/006tNbRwgy1fycc4hdvwtj30ft05mjtk.jpg)

**一个卷积神经网络各层应用实例**

![一个卷积神经网络各层应用实例](https://ws2.sinaimg.cn/large/006tNbRwgy1fycc6knv4qj30yv0gph1j.jpg)

上图中CNN要做的事情是：给定一张图片，是车还是马未知，是什么车也未知，现在需要模型判断这张图片里具体是一个什么东西，总之输出一个结果：如果是车 那是什么车。

我们按照从左到右的方向来理一下：

**左边：**

- 最左边是数据输入层，对数据做一些处理，比如去均值（把输入数据各个维度都中心化为0，避免数据过多偏差，影响训练效果）、归一化（把所有的数据都归一到同样的范围）、PCA/白化等等。CNN只对训练集做“去均值”这一步。

**中间：**

- CONV：卷积计算层，线性乘积求和。
- RELU：激励层，ReLU是激活函数的一种。
- POOL：池化层，简言之，即取区域平均或最大。

**右边：**

- FC：全连接层

### 卷积神经网络(CNN)中的局部感知和权重共享

#### CNN中的局部感知

在CNN中，滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据。这个过程中，有这么几个参数： 

* 深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。
* 步长stride：决定滑动多少步可以到边缘。
* 填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。

![CNN中的局部感知](https://ws2.sinaimg.cn/large/006tNbRwgy1fyccnqr48wj31ts0jcgw1.jpg)

上图就是一个典型的局部感知的示例图。其中黄色部分的矩阵为滤波器，深度为1，步长为1，填充值为0。很明显我们可以看出，每次滤波器都是针对某一局部的数据窗口进行卷积，这就是所谓的CNN中的局部感知机制。

那为什么要局部感知呢？

> 打个比方，滤波器就像一双眼睛，人类视角有限，一眼望去，只能看到这世界的局部。如果一眼就看到全世界，你会累死，而且一下子接受全世界所有信息，你大脑接收不过来。当然，即便是看局部，针对局部里的信息人类双眼也是有偏重、偏好的。比如看美女，对脸、胸、腿是重点关注，所以这3个输入的权重相对较大。

#### CNN中的权重共享

那么权重共享又是什么呢？还是拿上图举例，滤波器在滑动的过程中，输入在变化，但中间滤波器(filter)的权重（即每个神经元连接数据窗口的权重）是固定不变的，这个权重不变即所谓的CNN中的**权重（参数）共享**机制。

> 再打个比方，某人环游全世界，所看到的信息在变，但采集信息的双眼不变。btw，不同人的双眼看同一个局部信息所感受到的不同，即一千个读者有一千个哈姆雷特，所以不同的滤波器就像不同的双眼，不同的人有着不同的反馈结果。

#### 用一张动图诠释局部感知和权重共享

![一张动图诠释局部感知和权重共享](https://ws2.sinaimg.cn/large/006tNbRwgy1fycddeiqy0g30nw0j6dxg.gif)

我在搜集资料的过程中发现了这张图，第一感觉非常的酷，如果理解了局部感知和权重共享那这张图就不难看懂了。

相信你也会有一个疑问，上图中的输出结果1具体是怎么计算得到的呢？接下来我们来分解下上述动图，详细解释下计算过程。

首先是第一张：

 ![](https://ws1.sinaimg.cn/large/006tNbRwgy1fyce0ouqozj30k00gyjsi.jpg)

其实，计算过程类似wx + b，w对应滤波器Filter w0，x对应不同的数据窗口，b对应Bias b0，相当于滤波器Filter w0与一个个数据窗口相乘再求和后，最后加上Bias b0得到输出结果1，如下过程所示：

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fyce3ib6w4j304g02m0su.jpg)

> 1x0 + 1x0 + -1x0 + -1x0 + 0x0 + 1x1+-1x0 + -1x0 + 0x1

+

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fyce5s0fpxj304i02lgls.jpg)

> -1x0 + 0x0 + -1x0 + 0x0 + 0x1 + -1x1 + 1x0 + -1x0 + 0x2

+

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fyce6vor5tj304h02j3yp.jpg)

> 0x0 + 1x0 + 0x0 + 1x0 + 0x2 + 1x0 + 0x0 + -1x0 + 1x0

+

1(这里的1就是Bias b0)

=

1

然后滤波器Filter w0固定不变，数据窗口向右移动2步，继续做内积计算，得到0的输出结果

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fyceb9k7udj30k00h83zk.jpg)

最后，换做另外一个不同的滤波器Filter w1、不同的偏置Bias b1，再跟图中最左边的数据窗口做卷积，可得到另外一个不同的输出。
![](https://ws3.sinaimg.cn/large/006tNbRwgy1fycec0ulgnj30k00gyabb.jpg)

### 池化

池化，简言之，即取区域平均或最大，其目的是为了减少特征图。池化操作对每个深度切片独立，规模一般为 2＊2，相对于卷积层进行卷积运算，池化层进行的运算一般有以下几种： 
* 最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。 
* 均值池化（Mean Pooling）。取4个点的均值。 
* 高斯池化。借鉴高斯模糊的方法。不常用。 
* 可训练池化。训练函数 ff ，接受4个点为输入，出入1个点。不常用。

最常见的池化层是规模为2*2， 步幅为2，对输入的每个深度切片进行下采样。每个MAX操作对四个数进行，如下图所示：

![](https://ws1.sinaimg.cn/large/006tNbRwgy1fycee4nqg4j30lv0a80th.jpg)上图所展示的是取区域最大，即上图左边部分中 左上角2x2的矩阵中6最大，右上角2x2的矩阵中8最大，左下角2x2的矩阵中3最大，右下角2x2的矩阵中4最大，所以得到上图右边部分的结果：6 8 3 4。均值池化类似。

* 池化操作将保存**深度大小不变**。

* 如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（zero-padding）的方式补成2的倍数，然后再池化。

### 全连接层（Fully-connected layer）
全连接层和卷积层可以相互转换： 

* 对于任意一个卷积层，要把它变成全连接层只需要把权重变成一个巨大的矩阵，其中大部分都是0 除了一些特定区块（因为局部感知），而且好多区块的权值还相同（由于权重共享）。 
* 相反地，对于任何一个全连接层也可以变为卷积层。比如，一个$K＝4096$ 的全连接层，输入层大小为 $7∗7∗512$，它可以等效为一个$ F=7, P=0, S=1, K=4096$的卷积层。换言之，我们把 filter size 正好设置为整个输入层大小。

## 参考

1. [CNN笔记：通俗理解卷积神经网络](https://blog.csdn.net/v_JULY_v/article/details/51812459)

2. [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/) 
3. [卷积神经网络-维基百科](https://www.wikiwand.com/zh-hans/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C) 
4. [卷积特征提取](http://deeplearning.stanford.edu/wiki/index.php/%E5%8D%B7%E7%A7%AF%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96) 
5. [卷积神经网络全面解析](http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi)
6. [斯坦福机器学习公开课](http://52opencourse.com/139/coursera%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0-%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%85%AB%E8%AF%BE-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E7%A4%BA-neural-networks-representation)
7. [理解卷积](https://www.zhihu.com/question/22298352)