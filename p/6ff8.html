<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>卷积神经网络(CNN)的相关概念 | 斯是陋室</title><meta name="keywords" content="卷积神经网络"><meta name="author" content="YueYong"><meta name="copyright" content="YueYong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="传统神经网络存在的问题 说卷积神经网络前，我们要先说一下传统神经网络存在的一些问题，上图是一个典型的传统神经网络的示例图。设想一个场景，假设我们要训练的的样本图片是100x100（像素）的，那么整张图片总共就是有10000个像素，那么在定义一个 传统神经网络的时候，输入层(input layer)就需要有1w个神经元，那么如果我们的中间的隐藏层(hidden layer)也需要有1w个神经元，那">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络(CNN)的相关概念">
<meta property="og:url" content="https://blog.liangyueyong.cn/p/6ff8.html">
<meta property="og:site_name" content="斯是陋室">
<meta property="og:description" content="传统神经网络存在的问题 说卷积神经网络前，我们要先说一下传统神经网络存在的一些问题，上图是一个典型的传统神经网络的示例图。设想一个场景，假设我们要训练的的样本图片是100x100（像素）的，那么整张图片总共就是有10000个像素，那么在定义一个 传统神经网络的时候，输入层(input layer)就需要有1w个神经元，那么如果我们的中间的隐藏层(hidden layer)也需要有1w个神经元，那">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2021-06-10T11:50:25.307Z">
<meta property="article:modified_time" content="2021-06-10T11:50:25.307Z">
<meta property="article:author" content="YueYong">
<meta property="article:tag" content="卷积神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://blog.liangyueyong.cn/p/6ff8"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-06-10 19:50:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
   if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }
  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified
    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }const asideStatus = saveToLocal.get('aside-status')
if (asideStatus !== undefined) {
   if (asideStatus === 'hide') {
     document.documentElement.classList.add('hide-aside')
   } else {
     document.documentElement.classList.remove('hide-aside')
   }
}})()</script><meta name="generator" content="Hexo 5.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://s1.ax1x.com/2020/09/08/wQQcbq.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">163</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">50</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">斯是陋室</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><h1 class="post-title">卷积神经网络(CNN)的相关概念</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-06-10T11:50:25.307Z" title="发表于 2021-06-10 19:50:25">2021-06-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-06-10T11:50:25.307Z" title="更新于 2021-06-10 19:50:25">2021-06-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>8分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/p/6ff8.html#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/p/6ff8.html" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fzpytupuhoj30xa0d04dz.jpg"></p>
<h2 id="传统神经网络存在的问题"><a href="#传统神经网络存在的问题" class="headerlink" title="传统神经网络存在的问题"></a>传统神经网络存在的问题</h2><p><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fycb7xeo3hj30lz0asn4a.jpg" alt="传统神经网络"></p>
<p>说卷积神经网络前，我们要先说一下传统神经网络存在的一些问题，上图是一个典型的传统神经网络的示例图。设想一个场景，假设我们要训练的的样本图片是100x100（像素）的，那么整张图片总共就是有10000个像素，那么在定义一个 传统神经网络的时候，输入层(input layer)就需要有1w个神经元，那么如果我们的中间的隐藏层(hidden layer)也需要有1w个神经元，那么总共需要的参数（权值）就高达1亿个（1w*1w），试想一下，这还只是一张100x100的图片就需要这么多的参数，如果图片更大之后呢，可想而知整个神经网络的计算量有多恐怖。当然，一旦权重多了之后，则必须要有足够量的样本进行训练，否则就会出现<a target="_blank" rel="noopener" href="https://yueyong.info/p/f2b.html">过拟合</a>的现象。因此我们可以知道，传统神经网络有以下两个问题：</p>
<ul>
<li>权值太多，计算量太大</li>
<li>权值太多，如果没有大量样本支撑则会出现过拟合现象</li>
</ul>
<a id="more"></a>

<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><h4 id="什么是卷积？"><a href="#什么是卷积？" class="headerlink" title="什么是卷积？"></a>什么是卷积？</h4><p>在了解卷积神经网络之前我们需要知道什么是卷积。对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器(filter)，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fycciisv4oj30co06xt9q.jpg" alt="滤波器"></p>
<p>举个具体的例子。比如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fyccjacwonj30g70csafb.jpg"></p>
<p> 分解下上图</p>
<blockquote>
<p> <img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fycckt9eo4j302602ut8u.jpg">对应位置上是数字先相乘后相加<img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fyccld8qxbj302a02e0st.jpg"> = <img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fycclulpizj301f017744.jpg"></p>
</blockquote>
<p>中间滤波器filter与数据窗口做内积，其具体计算过程则是：4x0 + 0x0 + 0x0 + 0x0 + 0x1 + 0x1 + 0x0 + 0x1 + -4x2 = -8</p>
<h4 id="图像上的卷积"><a href="#图像上的卷积" class="headerlink" title="图像上的卷积"></a>图像上的卷积</h4><p>在下图对应的计算过程中，输入是一定区域大小(width*height)的数据，和滤波器filter（带着一组固定权重的神经元）做内积后等到新的二维数据。</p>
<p>如下图所示:</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fyccslh0vpj30hv0b1wq3.jpg"></p>
<p>具体来说，左边是图像输入，中间部分就是滤波器filter（带着一组固定权重的神经元），不同的滤波器filter会得到不同的输出数据，比如颜色深浅、轮廓。相当于如果想提取图像的不同特征，则用不同的滤波器filter，提取想要的关于图像的特定信息：颜色深浅或轮廓。用一句话解释不同滤波器之间的差异就是：<strong>一千个读者就有一千个哈姆雷特</strong></p>
<h3 id="什么是卷积神经网络？"><a href="#什么是卷积神经网络？" class="headerlink" title="什么是卷积神经网络？"></a>什么是卷积神经网络？</h3><p><strong>卷积神经网络（Convolutional Neural Network, CNN）</strong>是一种<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">前馈神经网络</a>，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。卷积神经网络与普通神经网络非常相似，它们都由具有可学习的权重和偏置常量(biases)的神经元组成。每个神经元都接收一些输入，并做一些点积计算，输出是每个分类的分数，普通神经网络里的一些计算技巧到这里依旧适用。但是卷积神经网络默认输入是图像，可以让我们把特定的性质编码入网络结构，使是我们的前馈函数更加有效率，并减少了大量参数。</p>
<p>**具有三维体积的神经元(3D volumes of neurons) **</p>
<p>卷积神经网络利用输入是图片的特点，把神经元设计成三个维度 ： width, height, depth(注意这个depth不是神经网络的深度，而是用来描述神经元的) 。比如输入的图片大小是 32 × 32 × 3 (rgb)，那么输入神经元就也具有 32×32×3 的维度。下面是图解：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNbRwgy1fycc4hdvwtj30ft05mjtk.jpg" alt="卷积神经网络"></p>
<p><strong>一个卷积神经网络各层应用实例</strong></p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fycc6knv4qj30yv0gph1j.jpg" alt="一个卷积神经网络各层应用实例"></p>
<p>上图中CNN要做的事情是：给定一张图片，是车还是马未知，是什么车也未知，现在需要模型判断这张图片里具体是一个什么东西，总之输出一个结果：如果是车 那是什么车。</p>
<p>我们按照从左到右的方向来理一下：</p>
<p><strong>左边：</strong></p>
<ul>
<li>最左边是数据输入层，对数据做一些处理，比如去均值（把输入数据各个维度都中心化为0，避免数据过多偏差，影响训练效果）、归一化（把所有的数据都归一到同样的范围）、PCA/白化等等。CNN只对训练集做“去均值”这一步。</li>
</ul>
<p><strong>中间：</strong></p>
<ul>
<li>CONV：卷积计算层，线性乘积求和。</li>
<li>RELU：激励层，ReLU是激活函数的一种。</li>
<li>POOL：池化层，简言之，即取区域平均或最大。</li>
</ul>
<p><strong>右边：</strong></p>
<ul>
<li>FC：全连接层</li>
</ul>
<h3 id="卷积神经网络-CNN-中的局部感知和权重共享"><a href="#卷积神经网络-CNN-中的局部感知和权重共享" class="headerlink" title="卷积神经网络(CNN)中的局部感知和权重共享"></a>卷积神经网络(CNN)中的局部感知和权重共享</h3><h4 id="CNN中的局部感知"><a href="#CNN中的局部感知" class="headerlink" title="CNN中的局部感知"></a>CNN中的局部感知</h4><p>在CNN中，滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据。这个过程中，有这么几个参数： </p>
<ul>
<li>深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。</li>
<li>步长stride：决定滑动多少步可以到边缘。</li>
<li>填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。</li>
</ul>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fyccnqr48wj31ts0jcgw1.jpg" alt="CNN中的局部感知"></p>
<p>上图就是一个典型的局部感知的示例图。其中黄色部分的矩阵为滤波器，深度为1，步长为1，填充值为0。很明显我们可以看出，每次滤波器都是针对某一局部的数据窗口进行卷积，这就是所谓的CNN中的局部感知机制。</p>
<p>那为什么要局部感知呢？</p>
<blockquote>
<p>打个比方，滤波器就像一双眼睛，人类视角有限，一眼望去，只能看到这世界的局部。如果一眼就看到全世界，你会累死，而且一下子接受全世界所有信息，你大脑接收不过来。当然，即便是看局部，针对局部里的信息人类双眼也是有偏重、偏好的。比如看美女，对脸、胸、腿是重点关注，所以这3个输入的权重相对较大。</p>
</blockquote>
<h4 id="CNN中的权重共享"><a href="#CNN中的权重共享" class="headerlink" title="CNN中的权重共享"></a>CNN中的权重共享</h4><p>那么权重共享又是什么呢？还是拿上图举例，滤波器在滑动的过程中，输入在变化，但中间滤波器(filter)的权重（即每个神经元连接数据窗口的权重）是固定不变的，这个权重不变即所谓的CNN中的<strong>权重（参数）共享</strong>机制。</p>
<blockquote>
<p>再打个比方，某人环游全世界，所看到的信息在变，但采集信息的双眼不变。btw，不同人的双眼看同一个局部信息所感受到的不同，即一千个读者有一千个哈姆雷特，所以不同的滤波器就像不同的双眼，不同的人有着不同的反馈结果。</p>
</blockquote>
<h4 id="用一张动图诠释局部感知和权重共享"><a href="#用一张动图诠释局部感知和权重共享" class="headerlink" title="用一张动图诠释局部感知和权重共享"></a>用一张动图诠释局部感知和权重共享</h4><p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fycddeiqy0g30nw0j6dxg.gif" alt="一张动图诠释局部感知和权重共享"></p>
<p>我在搜集资料的过程中发现了这张图，第一感觉非常的酷，如果理解了局部感知和权重共享那这张图就不难看懂了。</p>
<p>相信你也会有一个疑问，上图中的输出结果1具体是怎么计算得到的呢？接下来我们来分解下上述动图，详细解释下计算过程。</p>
<p>首先是第一张：</p>
<p> <img src="https://ws1.sinaimg.cn/large/006tNbRwgy1fyce0ouqozj30k00gyjsi.jpg"></p>
<p>其实，计算过程类似wx + b，w对应滤波器Filter w0，x对应不同的数据窗口，b对应Bias b0，相当于滤波器Filter w0与一个个数据窗口相乘再求和后，最后加上Bias b0得到输出结果1，如下过程所示：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fyce3ib6w4j304g02m0su.jpg"></p>
<blockquote>
<p>1x0 + 1x0 + -1x0 + -1x0 + 0x0 + 1x1+-1x0 + -1x0 + 0x1</p>
</blockquote>
<p>+</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNbRwgy1fyce5s0fpxj304i02lgls.jpg"></p>
<blockquote>
<p>-1x0 + 0x0 + -1x0 + 0x0 + 0x1 + -1x1 + 1x0 + -1x0 + 0x2</p>
</blockquote>
<p>+</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fyce6vor5tj304h02j3yp.jpg"></p>
<blockquote>
<p>0x0 + 1x0 + 0x0 + 1x0 + 0x2 + 1x0 + 0x0 + -1x0 + 1x0</p>
</blockquote>
<p>+</p>
<p>1(这里的1就是Bias b0)</p>
<p>=</p>
<p>1</p>
<p>然后滤波器Filter w0固定不变，数据窗口向右移动2步，继续做内积计算，得到0的输出结果</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fyceb9k7udj30k00h83zk.jpg"></p>
<p>最后，换做另外一个不同的滤波器Filter w1、不同的偏置Bias b1，再跟图中最左边的数据窗口做卷积，可得到另外一个不同的输出。<br><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fycec0ulgnj30k00gyabb.jpg"></p>
<h3 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h3><p>池化，简言之，即取区域平均或最大，其目的是为了减少特征图。池化操作对每个深度切片独立，规模一般为 2＊2，相对于卷积层进行卷积运算，池化层进行的运算一般有以下几种： </p>
<ul>
<li>最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。 </li>
<li>均值池化（Mean Pooling）。取4个点的均值。 </li>
<li>高斯池化。借鉴高斯模糊的方法。不常用。 </li>
<li>可训练池化。训练函数 ff ，接受4个点为输入，出入1个点。不常用。</li>
</ul>
<p>最常见的池化层是规模为2*2， 步幅为2，对输入的每个深度切片进行下采样。每个MAX操作对四个数进行，如下图所示：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNbRwgy1fycee4nqg4j30lv0a80th.jpg">上图所展示的是取区域最大，即上图左边部分中 左上角2x2的矩阵中6最大，右上角2x2的矩阵中8最大，左下角2x2的矩阵中3最大，右下角2x2的矩阵中4最大，所以得到上图右边部分的结果：6 8 3 4。均值池化类似。</p>
<ul>
<li><p>池化操作将保存<strong>深度大小不变</strong>。</p>
</li>
<li><p>如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（zero-padding）的方式补成2的倍数，然后再池化。</p>
</li>
</ul>
<h3 id="全连接层（Fully-connected-layer）"><a href="#全连接层（Fully-connected-layer）" class="headerlink" title="全连接层（Fully-connected layer）"></a>全连接层（Fully-connected layer）</h3><p>全连接层和卷积层可以相互转换： </p>
<ul>
<li>对于任意一个卷积层，要把它变成全连接层只需要把权重变成一个巨大的矩阵，其中大部分都是0 除了一些特定区块（因为局部感知），而且好多区块的权值还相同（由于权重共享）。 </li>
<li>相反地，对于任何一个全连接层也可以变为卷积层。比如，一个$K＝4096$ 的全连接层，输入层大小为 $7∗7∗512$，它可以等效为一个$ F=7, P=0, S=1, K=4096$的卷积层。换言之，我们把 filter size 正好设置为整个输入层大小。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/51812459">CNN笔记：通俗理解卷积神经网络</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://cs231n.github.io/convolutional-networks/">CS231n: Convolutional Neural Networks for Visual Recognition</a> </p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.wikiwand.com/zh-hans/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">卷积神经网络-维基百科</a> </p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://deeplearning.stanford.edu/wiki/index.php/%E5%8D%B7%E7%A7%AF%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96">卷积特征提取</a> </p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://www.moonshile.com/post/juan-ji-shen-jing-wang-luo-quan-mian-jie-xi">卷积神经网络全面解析</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://52opencourse.com/139/coursera%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0-%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%85%AB%E8%AF%BE-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E7%A4%BA-neural-networks-representation">斯坦福机器学习公开课</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/22298352">理解卷积</a></p>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">YueYong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.liangyueyong.cn/p/6ff8.html">https://blog.liangyueyong.cn/p/6ff8.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.liangyueyong.cn" target="_blank">斯是陋室</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">卷积神经网络</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/p/f25e.html"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Java 并查集的实现</div></div></a></div><div class="next-post pull-right"><a href="/p/ab06.html"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Flutter数据存储之shared_preferences</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside_content" id="aside_content"><div class="card-widget card-info"><div class="card-content"><div class="card-info-avatar is-center"><img class="avatar-img" src="https://s1.ax1x.com/2020/09/08/wQQcbq.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">YueYong</div><div class="author-info__description">凡是过往，皆为序章</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">163</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">50</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/YueYongDEV"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/YueYongDEV" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:yueyong1030@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div></div><div class="card-widget card-announcement"><div class="card-content"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">微信公众号「01二进制」</div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="card-content"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.</span> <span class="toc-text">传统神经网络存在的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.</span> <span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF"><span class="toc-number">2.1.</span> <span class="toc-text">卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%B7%E7%A7%AF%EF%BC%9F"><span class="toc-number">2.1.1.</span> <span class="toc-text">什么是卷积？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E4%B8%8A%E7%9A%84%E5%8D%B7%E7%A7%AF"><span class="toc-number">2.1.2.</span> <span class="toc-text">图像上的卷积</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9F"><span class="toc-number">2.2.</span> <span class="toc-text">什么是卷积神经网络？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-CNN-%E4%B8%AD%E7%9A%84%E5%B1%80%E9%83%A8%E6%84%9F%E7%9F%A5%E5%92%8C%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB"><span class="toc-number">2.3.</span> <span class="toc-text">卷积神经网络(CNN)中的局部感知和权重共享</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CNN%E4%B8%AD%E7%9A%84%E5%B1%80%E9%83%A8%E6%84%9F%E7%9F%A5"><span class="toc-number">2.3.1.</span> <span class="toc-text">CNN中的局部感知</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CNN%E4%B8%AD%E7%9A%84%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB"><span class="toc-number">2.3.2.</span> <span class="toc-text">CNN中的权重共享</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%A8%E4%B8%80%E5%BC%A0%E5%8A%A8%E5%9B%BE%E8%AF%A0%E9%87%8A%E5%B1%80%E9%83%A8%E6%84%9F%E7%9F%A5%E5%92%8C%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB"><span class="toc-number">2.3.3.</span> <span class="toc-text">用一张动图诠释局部感知和权重共享</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96"><span class="toc-number">2.4.</span> <span class="toc-text">池化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%88Fully-connected-layer%EF%BC%89"><span class="toc-number">2.5.</span> <span class="toc-text">全连接层（Fully-connected layer）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">3.</span> <span class="toc-text">参考</span></a></li></ol></div></div></div><div class="card-widget card-recent-post"><div class="card-content"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/p/c0e2.html" title="给编译器看的注释——「注解」"><img src="https://cdn.ytools.xyz/uPic/NS7lqsiShot2021-06-12%2000.58.18.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="给编译器看的注释——「注解」"/></a><div class="content"><a class="title" href="/p/c0e2.html" title="给编译器看的注释——「注解」">给编译器看的注释——「注解」</a><time datetime="2021-06-14T07:23:23.088Z" title="发表于 2021-06-14 15:23:23">2021-06-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/ec8b.html" title="未雨绸缪，小米前端实习面经"><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdk2ny4uugj30xo0he0yu.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="未雨绸缪，小米前端实习面经"/></a><div class="content"><a class="title" href="/p/ec8b.html" title="未雨绸缪，小米前端实习面经">未雨绸缪，小米前端实习面经</a><time datetime="2021-06-10T11:50:25.448Z" title="发表于 2021-06-10 19:50:25">2021-06-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/e834.html" title="计算机相关专业实习指北"><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdquuna43wj30zk0quaf8.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机相关专业实习指北"/></a><div class="content"><a class="title" href="/p/e834.html" title="计算机相关专业实习指北">计算机相关专业实习指北</a><time datetime="2021-06-10T11:50:25.448Z" title="发表于 2021-06-10 19:50:25">2021-06-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/4fa2.html" title="搏一搏，单车变摩托，记录一下我的淘宝实习面试"><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdk77kyd9vj30k40budh9.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="搏一搏，单车变摩托，记录一下我的淘宝实习面试"/></a><div class="content"><a class="title" href="/p/4fa2.html" title="搏一搏，单车变摩托，记录一下我的淘宝实习面试">搏一搏，单车变摩托，记录一下我的淘宝实习面试</a><time datetime="2021-06-10T11:50:25.447Z" title="发表于 2021-06-10 19:50:25">2021-06-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/p/d04f.html" title="刚完一波蚂蚁金服的面试后，他说他累了"><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdlnjvgiotj30x20kytfw.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="刚完一波蚂蚁金服的面试后，他说他累了"/></a><div class="content"><a class="title" href="/p/d04f.html" title="刚完一波蚂蚁金服的面试后，他说他累了">刚完一波蚂蚁金服的面试后，他说他累了</a><time datetime="2021-06-10T11:50:25.446Z" title="发表于 2021-06-10 19:50:25">2021-06-10</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2017 - 2021 By YueYong</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    let initData = {
      el: '#vcomment',
      appId: 'U0W7ATmVTjBxWkHt96LR9Bj0-gzGzoHsz',
      appKey: 'uko1Os2bkFSID34qVRrgjCcX',
      placeholder: '记得留下你的昵称和邮箱....可以快速收到回复',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: true,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: {"tv_doge":"6ea59c827c414b4a2955fe79e0f6fd3dcd515e24.png","tv_親親":"a8111ad55953ef5e3be3327ef94eb4a39d535d06.png","tv_偷笑":"bb690d4107620f1c15cff29509db529a73aee261.png","tv_再見":"180129b8ea851044ce71caf55cc8ce44bd4a4fc8.png","tv_冷漠":"b9cbc755c2b3ee43be07ca13de84e5b699a3f101.png","tv_發怒":"34ba3cd204d5b05fec70ce08fa9fa0dd612409ff.png","tv_發財":"34db290afd2963723c6eb3c4560667db7253a21a.png","tv_可愛":"9e55fd9b500ac4b96613539f1ce2f9499e314ed9.png","tv_吐血":"09dd16a7aa59b77baa1155d47484409624470c77.png","tv_呆":"fe1179ebaa191569b0d31cecafe7a2cd1c951c9d.png","tv_嘔吐":"9f996894a39e282ccf5e66856af49483f81870f3.png","tv_困":"241ee304e44c0af029adceb294399391e4737ef2.png","tv_壞笑":"1f0b87f731a671079842116e0991c91c2c88645a.png","tv_大佬":"093c1e2c490161aca397afc45573c877cdead616.png","tv_大哭":"23269aeb35f99daee28dda129676f6e9ea87934f.png","tv_委屈":"d04dba7b5465779e9755d2ab6f0a897b9b33bb77.png","tv_害羞":"a37683fb5642fa3ddfc7f4e5525fd13e42a2bdb1.png","tv_尷尬":"7cfa62dafc59798a3d3fb262d421eeeff166cfa4.png","tv_微笑":"70dc5c7b56f93eb61bddba11e28fb1d18fddcd4c.png","tv_思考":"90cf159733e558137ed20aa04d09964436f618a1.png","tv_驚嚇":"0d15c7e2ee58e935adc6a7193ee042388adc22af.png"},
      enableQQ: true,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('nick,mail'.split(','))
    }
    
    if (false) {
      const otherData = false
      initData = Object.assign({}, initData, otherData)
    }
    
    const valine = new Valine(initData)
  }

  if (typeof Valine === 'function') initValine() 
  else $.getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.querySelector('#vcomment'),loadValine)
  else setTimeout(() => loadValine(), 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>